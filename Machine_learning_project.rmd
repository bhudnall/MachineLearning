## Machine Learning Course Project

#### Title: Machine Learning Analysis by Brian Hudnall

#### Project Summary and Goals: 6 young health participants perform one set of 10 repetitions of unilateral bicep curls 5 different ways. Predict the manner in which someone did the exercise ("classe") variable in data set. Describe how to build the model, do cross validation, and what the expected out of sample error is for the final model.

#### Results: After doing extensive data cleaning and pre processing the final model (random forest) was able to succesfully predict how each testing set participant performed the exercise. The accuracy of the final model was 99%, so the expected out of sample error rate is extremely small.

#### Load Data:

First, load the necessary packages used in analysis. Set the correct working directory and then read in the data using the readr package.

```{r message=FALSE, warning=FALSE}
require(readr)
require(ggplot2)
require(caret)
require(dplyr)
require(stringr)
require(rattle)
require(rpart.plot)
require(randomForest)
require(AppliedPredictiveModeling)
```

```{r}
## Uploading the data set and cleaning
training <- read_csv("pml-training.csv")
testing <- read_csv("pml-testing.csv")
```

#### Clean the data:

The user name and time stamp fields are unecessary so they are removed all together from both data sets. Also, there are many fields in the training set that are not in the testing set. Because these fields will not be used in the final prediction, they are removed.

```{r}
## Remove the first columns and other unecessary columns
training <- select(training
                   , -user_name
                   , -cvtd_timestamp
                   , -raw_timestamp_part_1
                   , -raw_timestamp_part_2
                   , -1)
testing <- select(testing
                  , -user_name
                  , -cvtd_timestamp
                  , -raw_timestamp_part_1
                  , -raw_timestamp_part_2
                  , -1)

# if sum of NA by col is less than number of rows than keep it 
testing <- testing[,colSums(is.na(testing)) < nrow(testing)]
testingColNames <- colnames(testing)
## Remove problem_id from the testingColNames set
testingColNames <- testingColNames[-length(testingColNames)]
## add in the classe variable from the training set
testingColNames <- append(testingColNames
                          , names(training)[length(names(training))])
## Find the training columns indexes from the testingColNames
colNums <- match(testingColNames, names(training))
## Filter down the columns based on the values in the training set
training <- select(training, colNums)
```

#### Explore, partition and pre-proces the data for model preparation:

Use the glimpse and summary function to look at the final sets before partitioning. Also check to make sure there are no NA values. From there, partition the data sets and have 60% of the data in the training set with 40% in the testing set. 
Then, do a near zero variance analysis to find variables with low variance and as a result can be remved because they are not a primary predictor. Finally, do somewhat of a prinicipal component analysis and find combinations of variables that have higher than a 95% correlation -- remove these variables as they will have a similar predictor effect and are not necessary in the final model.

```{r results="hide"}
## Explore the dataset
glimpse(training)
summary(training)
```

```{r}
## look for any NA values for imputation if needed
## Also look for near zero covariates for removal
sum(is.na(training))

## PARTITION DATA

inTrain <- createDataPartition(y=training$classe,
                               p=0.6, list=FALSE)
training_training <- training[inTrain,]
training_testing <- training[-inTrain,]
dim(training_training)
dim(training_testing)

## PREPROCESSING

## Near Zero Variance Analysis, remove new_window as it has no variance
nzv <- nearZeroVar(training_training, saveMetrics=TRUE)
training_training <- select(training_training, -new_window)
training_testing <- select(training_testing, -new_window)
testing <- select(testing, -new_window)

## Remove highly correlated predictors. First by calc a correlation matrix
## and then removing values that are highly correlated
descrCor <- cor(training_training[,-54])
highCorr <- sum(abs(descrCor[upper.tri(descrCor)]) > .95)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .95)
training_training_filt <- training_training[,-highlyCorDescr]
training_testing_filt <- training_testing[,-highlyCorDescr]
testing_filt <- testing[,-highlyCorDescr]

```


#### Run the data models:

First, run a basic decision tree model to see what the primary predictors are. After creating the model fit and running it on the test set, the model shows that the accuracy level is at a decent 71% -- this can definitely be optimized. Then run a random forest model which essentially bootstraps samples of decision trees to pick the most accurate sample. The final model shows a 99% accuracy level. The in-sample error is very low, leading us to believe that the out of sample error will be slightly worse, but enough to give us an accurate prediction. The last step is to run this model on the true test set.  

```{r}
modFit <- rpart(classe ~ ., data=training_training_filt, method="class")
fancyRpartPlot(modFit)

training_testing_predict <- predict(modFit
                                    , training_testing_filt
                                    , type = "class")

confusionMatrix(training_testing_predict
                , training_testing_filt$classe)

training_training_filt$classe <- as.factor(training_training_filt$classe)
training_testing_filt$classe <- as.factor(training_testing_filt$classe)

modFit_rf <- randomForest(classe ~ ., data=training_training_filt)
rf_predictions <- predict(modFit_rf, training_testing_filt, type = "class")
confusionMatrix(rf_predictions, training_testing_filt$classe)

final_predictions <- predict(modFit_rf, testing_filt, type = "class")

```

#### Print predictions:

```{r}
pml_write_files = function(x){
    n = length(x)
    for(i in 1:n){
        filename = paste0("problem_id_",i,".txt")
        write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
}

pml_write_files(final_predictions)
```


    